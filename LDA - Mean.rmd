LDA Assumptions
```{r}
library(heplots)
boxm <- heplots::boxM(assump[,2:11], heart$HeartDiseaseorAttack)
boxm
plot(boxm)


heart.yes <- subset(heart, HeartDiseaseorAttack == 1)
heart.no <- subset(heart, HeartDiseaseorAttack == 0)

qqnorm(heart.yes[["BMI"]]); qqline(heart.yes[["BMI"]], col = 2) 

qqnorm(heart.no[["BMI"]]); qqline(heart.no[["BMI"]], col = 2)
```



LDA Importance
```{r}
library(caret)
set.seed(1234)

intermed <- heart_no_na
levels(intermed$HeartDiseaseorAttack) <- c("zero", "one")

ctrl <- trainControl(classProbs = TRUE)
#ctrl1 <- trainControl(summaryFunction = twoClassSummary, classProbs = TRUE)

lda_mod <- train(HeartDiseaseorAttack~., data = intermed, method = "lda", trControl = ctrl, metric = "Accuracy")

varImp(lda_mod)
```




Cross Validated Predictions
```{r, warning = FALSE}
set.seed(1234)
K = 10
fold = createFolds(1:nrow(heart_no_na), k = K, list = FALSE)
CV_err = 0
predictions = rep(NA, nrow(heart_no_na))

re_indexed_heart <- heart_no_na
rownames(re_indexed_heart) <- 1:nrow(re_indexed_heart)

for(i in 1:K){
  test_ind <- (fold == i)
  train <- re_indexed_heart[!test_ind,]
  test <- re_indexed_heart[test_ind,]
  
  lda <- lda(HeartDiseaseorAttack~., data = train)
  
  for(j in row.names(test)){
    predictions[as.integer(j)] <- predict(lda, newdata = test[j,])$posterior[,2]
  }
  
}
```



AUC/Accuracy Maximization
```{r, warning = FALSE}
set.seed(1234)
library(pROC)
thresholds <- seq(0.01, 0.9, 0.01)
auc_vals <- vector("numeric", length(thresholds))
accuracy <- vector("numeric", length(thresholds))

for (i in 1:length(thresholds)){
  pred <- ifelse(predictions > thresholds[i], 1, 0)
  
  accuracy[i] <- mean(pred != (as.numeric(heart_no_na$HeartDiseaseorAttack)-1))
  
  myroc <- roc((as.numeric(heart_no_na$HeartDiseaseorAttack)-1), pred)
  auc_vals[i] <- myroc$auc
}

which(auc_vals == max(auc_vals))
auc_vals[which(auc_vals == max(auc_vals))]
# 0.06
# 0.7391341

which(accuracy == min(accuracy))
accuracy[which(accuracy == min(accuracy))]
# 83
# 0.08932384
```


# If we wanted to truly find the optimal decision threshold we would need to consider the specific costs associated with false positives and false negatives.

Chosen Threshold/Accuracy Test
```{r}
set.seed(1234)
pred <- ifelse(predictions > 0.08146426, 1, 0)
  
accuracy_final <- mean((pred - (as.numeric(heart_no_na$HeartDiseaseorAttack)-1))^2)
  
myroc_final <- roc((as.numeric(heart_no_na$HeartDiseaseorAttack)-1), pred)
  
  
accuracy_final
myroc_final$auc

plot(myroc_final)
table(pred, heart_no_na$HeartDiseaseorAttack)

# 0.5 Threshold -> 0.09697509 accuracy & 0.5568 AUC
# 0.06 Threshold -> 0.3354093 accuracy & 0.736 AUC
```


Total Model ROC and AUC
```{r}
library(ROCR)

pred <- prediction(predictions, heart_no_na$HeartDiseaseorAttack)
perf <- performance(pred, "tpr", "fpr")

plot(perf, colorize = TRUE, main = "LDA (Iterative Imputation)")
auc <- performance(pred, "auc")@y.values[[1]]
auc

df <- data.frame(cut = perf@alpha.values[[1]], sens = perf@y.values[[1]], spec = perf@x.values[[1]])
df$add <- df$sens + df$spec
df$one <- abs(df$add - 1)
df$distance <- sqrt((1 - df$sens)^2 + (0 - df$spec)^2)
df$spec <- 1 - df$spec

# Returns the optimal cutpoint for maximize AUC?
df[order(df$distance, decreasing = FALSE),][1,]
```
